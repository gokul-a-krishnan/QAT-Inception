{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data is not disclosed since its given by Client.\n",
    "# Some parts of code may not available but you get the idea about QAT using TF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /home/gk/python-envs/scratch/lib/python3.11/site-packages (0.7.5)\r\n",
      "Requirement already satisfied: absl-py~=1.2 in /home/gk/python-envs/scratch/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.4.0)\r\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/gk/python-envs/scratch/lib/python3.11/site-packages (from tensorflow-model-optimization) (0.1.8)\r\n",
      "Requirement already satisfied: numpy~=1.23 in /home/gk/python-envs/scratch/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.24.3)\r\n",
      "Requirement already satisfied: six~=1.14 in /home/gk/python-envs/scratch/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-model-optimization"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXHmEk-PUx5s",
    "outputId": "17668911-bfe8-4f8b-9174-734de21f3359",
    "ExecuteTime": {
     "end_time": "2023-09-15T16:52:03.384121890Z",
     "start_time": "2023-09-15T16:52:02.491370477Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6fgvvXhVKkB",
    "outputId": "dd572cc7-06e2-489c-8583-b6be18cbda4a"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from os.path import join\n",
    "from os import listdir\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import get_file, Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.image import resize\n",
    "from tensorflow import sqrt\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "\n",
    "import tensorflow_model_optimization as tfmot"
   ],
   "metadata": {
    "id": "Oeh51fSV-JjP",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:14.122637086Z",
     "start_time": "2023-09-15T17:02:12.654543860Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 22:32:12.890616: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-15 22:32:12.921576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_root = \"data\"\n",
    "images_path = join(data_root, \"images\")\n",
    "labels_path = join(data_root, \"labels\")\n",
    "\n",
    "images_train_path = join(images_path, \"train\")\n",
    "images_val_path = join(images_path, \"val\")\n",
    "\n",
    "labels_train_path = join(labels_path, \"train\")\n",
    "labels_val_path = join(labels_path, \"val\")"
   ],
   "metadata": {
    "id": "feaLH5QQVGb6",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:14.125674374Z",
     "start_time": "2023-09-15T17:02:14.122849048Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_obj = 30\n",
    "image_size = (299, 299)\n",
    "input_shape = (299, 299, 3)\n",
    "retrain_baseline = False\n",
    "no_output_neurons = max_obj * 5\n",
    "batch_size = 16"
   ],
   "metadata": {
    "id": "l8419IqaVSwl",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:14.130917993Z",
     "start_time": "2023-09-15T17:02:14.124933210Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def process(img_path, lbl_path):\n",
    "    labels = listdir(lbl_path)\n",
    "    xy = []\n",
    "    for i in range(len(labels)):\n",
    "        train_label = labels[i]\n",
    "        train_image = (train_label[:-4] + \".jpg\")\n",
    "        annotation_file_path = join(lbl_path, train_label)\n",
    "        image_file_path = join(img_path, train_image)\n",
    "        with open(annotation_file_path, \"r\") as annotation_file:\n",
    "            annotations = [([float(j) for j in i.strip().split(\" \")]) for i in annotation_file.readlines()]\n",
    "            for annotation in annotations:\n",
    "                annotation[0] = 1  # TODO since there is only one class\n",
    "            while len(annotations) < max_obj:\n",
    "                annotations.append([0, 0, 0, 0, 0])\n",
    "            if len(annotations) > max_obj:\n",
    "                annotations = annotations[:max_obj]\n",
    "        annotation = np.array(annotations)\n",
    "        xy.append([image_file_path, annotation.flatten()])\n",
    "    return xy\n"
   ],
   "metadata": {
    "id": "Vi6cWRu5VcKH",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:14.176261106Z",
     "start_time": "2023-09-15T17:02:14.131028118Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_image(image_path, dsize):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, dsize)\n",
    "    image = image.astype(float)\n",
    "    image /= 255.0\n",
    "    return image"
   ],
   "metadata": {
    "id": "VHEigPTmVezR",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:14.176443618Z",
     "start_time": "2023-09-15T17:02:14.174660652Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, data, input_size, batch_size, shuffle=False):\n",
    "        self.data = data\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(data)\n",
    "        pass\n",
    "\n",
    "    def __get_input(self, path, target_size):\n",
    "        print(path)\n",
    "        image = load_img(path)\n",
    "        image_arr = img_to_array(image)\n",
    "        image_arr = resize(image_arr, (target_size[0], target_size[1])).numpy()\n",
    "        return image_arr / 255.\n",
    "\n",
    "    def __get_data(self, batches):\n",
    "        path_batch = [i[0] for i in batches]\n",
    "        y_batch = np.asarray([i[1] for i in batches])\n",
    "        x_batch = np.asarray([self.__get_input(i, self.input_size) for i in path_batch])\n",
    "\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batches = self.data[index * self.batch_size:(index + 1) * batch_size]\n",
    "        X, y = self.__get_data(batches)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n"
   ],
   "metadata": {
    "id": "LD9yaZFpVhsG",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:44:29.547988173Z",
     "start_time": "2023-09-15T17:44:29.502905535Z"
    }
   },
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = process(images_train_path, labels_train_path)\n",
    "val_dataset = process(images_val_path, labels_val_path)\n",
    "\n",
    "train_generator = CustomDataGenerator(train_dataset, image_size, batch_size)\n",
    "val_generator = CustomDataGenerator(val_dataset, image_size, batch_size)"
   ],
   "metadata": {
    "id": "vjDmD2SLWWRH",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:44:31.878572269Z",
     "start_time": "2023-09-15T17:44:31.855744269Z"
    }
   },
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def conv2d_bn(\n",
    "        x, filters, num_row, num_col, padding=\"same\", strides=(1, 1), name=None\n",
    "):\n",
    "    if name is not None:\n",
    "        bn_name = name + \"_bn\"\n",
    "        conv_name = name + \"_conv\"\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "    x = Conv2D(\n",
    "        filters,\n",
    "        (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=conv_name,\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=3, scale=False, name=bn_name)(x)\n",
    "    x = Activation(\"relu\", name=name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def InceptionV3FeatureExtractor(input_shape):\n",
    "    channel_axis = 3\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "    x = conv2d_bn(x, 32, 3, 3, padding=\"valid\")\n",
    "    x = conv2d_bn(x, 64, 3, 3)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv2d_bn(x, 80, 1, 1, padding=\"valid\")\n",
    "    x = conv2d_bn(x, 192, 3, 3, padding=\"valid\")\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # mixed 0: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "    x = concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed0\")\n",
    "\n",
    "    # mixed 1: 35 x 35 x 288\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed1\")\n",
    "\n",
    "    # mixed 2: 35 x 35 x 288\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed2\")\n",
    "\n",
    "    # mixed 3: 17 x 17 x 768\n",
    "    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3, strides=(2, 2),\n",
    "                             padding=\"valid\")\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = concatenate([branch3x3, branch3x3dbl, branch_pool], axis=channel_axis,\n",
    "                    name=\"mixed3\")\n",
    "\n",
    "    # mixed 4: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed4\")\n",
    "\n",
    "    # mixed 5, 6: 17 x 17 x 768\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "        branch7x7 = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "        branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "                        axis=channel_axis, name=\"mixed\" + str(5 + i))\n",
    "\n",
    "    # mixed 7: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed7\")\n",
    "\n",
    "    # mixed 8: 8 x 8 x 1280\n",
    "    branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 3, 3, strides=(2, 2),\n",
    "                            padding=\"valid\")\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = concatenate([branch3x3, branch7x7x3, branch_pool], axis=channel_axis,\n",
    "                    name=\"mixed8\")\n",
    "\n",
    "    # mixed 9: 8 x 8 x 2048\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "        branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "        branch3x3 = concatenate([branch3x3_1, branch3x3_2], axis=channel_axis,\n",
    "                                name=\"mixed9_\" + str(i), )\n",
    "\n",
    "        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "        branch3x3dbl = concatenate([branch3x3dbl_1, branch3x3dbl_2],\n",
    "                                   axis=channel_axis)\n",
    "\n",
    "        branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = concatenate([branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "                        axis=channel_axis, name=\"mixed\" + str(9 + i))\n",
    "\n",
    "    # # classification layer\n",
    "    # x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "    # x = Dense(\n",
    "    #     1000, activation=\"softmax\", name=\"predictions\"\n",
    "    # )(x)\n",
    "    # model = Model(img_input, x, name=\"inception_v3\")\n",
    "\n",
    "    WEIGHTS_PATH = \"https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "    weights_path = get_file(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", WEIGHTS_PATH,\n",
    "                            cache_subdir=\"models\", file_hash=\"bcbd6486424b2319ff4ef7d526e38f63\")\n",
    "\n",
    "    model = Model(img_input, x, name=\"inception_v3\")\n",
    "    model.load_weights(weights_path)\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "75sy7K4sB4Qa",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:14.670772051Z",
     "start_time": "2023-09-15T17:02:14.626959117Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def InceptionV3(input_shape):\n",
    "    channel_axis = 3\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "    x = conv2d_bn(x, 32, 3, 3, padding=\"valid\")\n",
    "    x = conv2d_bn(x, 64, 3, 3)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv2d_bn(x, 80, 1, 1, padding=\"valid\")\n",
    "    x = conv2d_bn(x, 192, 3, 3, padding=\"valid\")\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # mixed 0: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "    x = concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed0\")\n",
    "\n",
    "    # mixed 1: 35 x 35 x 288\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed1\")\n",
    "\n",
    "    # mixed 2: 35 x 35 x 288\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed2\")\n",
    "\n",
    "    # mixed 3: 17 x 17 x 768\n",
    "    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3, strides=(2, 2),\n",
    "                             padding=\"valid\")\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = concatenate([branch3x3, branch3x3dbl, branch_pool], axis=channel_axis,\n",
    "                    name=\"mixed3\")\n",
    "\n",
    "    # mixed 4: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed4\")\n",
    "\n",
    "    # mixed 5, 6: 17 x 17 x 768\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "        branch7x7 = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "        branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "                        axis=channel_axis, name=\"mixed\" + str(5 + i))\n",
    "\n",
    "    # mixed 7: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "                    axis=channel_axis, name=\"mixed7\")\n",
    "\n",
    "    # mixed 8: 8 x 8 x 1280\n",
    "    branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3, strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 3, 3, strides=(2, 2),\n",
    "                            padding=\"valid\")\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = concatenate([branch3x3, branch7x7x3, branch_pool], axis=channel_axis,\n",
    "                    name=\"mixed8\")\n",
    "\n",
    "    # mixed 9: 8 x 8 x 2048\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "        branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "        branch3x3 = concatenate([branch3x3_1, branch3x3_2], axis=channel_axis,\n",
    "                                name=\"mixed9_\" + str(i), )\n",
    "\n",
    "        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "        branch3x3dbl = concatenate([branch3x3dbl_1, branch3x3dbl_2],\n",
    "                                   axis=channel_axis)\n",
    "\n",
    "        branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = concatenate([branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "                        axis=channel_axis, name=\"mixed\" + str(9 + i))\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(no_output_neurons, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(img_input, x, name=\"inception_v3\")\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "8zhF3MLvRLXN",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:14.826054970Z",
     "start_time": "2023-09-15T17:02:14.815885497Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "f_model = InceptionV3FeatureExtractor((299, 299, 3))"
   ],
   "metadata": {
    "id": "jMgL_WbYMHNU",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:16.783927495Z",
     "start_time": "2023-09-15T17:02:15.222648812Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 22:32:15.054455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.073336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.073456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.074230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.074332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.074418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.125332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.125437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.125510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 22:32:15.125574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10002 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = InceptionV3((299, 299, 3))"
   ],
   "metadata": {
    "id": "q1a1fjXfRceY",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:18.106204259Z",
     "start_time": "2023-09-15T17:02:16.775717102Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "weights_list = f_model.get_weights()\n",
    "\n",
    "for i, l in enumerate(f_model.layers):\n",
    "    w = f_model.layers[i].weights\n",
    "    model.layers[i].set_weights(w)"
   ],
   "metadata": {
    "id": "yvs6QDcuRivq",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:18.525968520Z",
     "start_time": "2023-09-15T17:02:18.105813074Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))"
   ],
   "metadata": {
    "id": "bghoREqRWjgR",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:18.534898333Z",
     "start_time": "2023-09-15T17:02:18.525769167Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "quantize_model = tfmot.quantization.keras.quantize_model"
   ],
   "metadata": {
    "id": "G9tmzwVtXd24",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:18.544040589Z",
     "start_time": "2023-09-15T17:02:18.530265958Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q_model = quantize_model(model)"
   ],
   "metadata": {
    "id": "yCWjIvg_Xg_X",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:26.324910841Z",
     "start_time": "2023-09-15T17:02:18.538415399Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q_model.compile(optimizer=\"adam\", loss=rmse, metrics=[\"mae\"])"
   ],
   "metadata": {
    "id": "ldn8QBONXn5n",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:26.445739203Z",
     "start_time": "2023-09-15T17:02:26.332764168Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q_model.summary()"
   ],
   "metadata": {
    "id": "kPdGhPzZXpKn",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:02:26.786097017Z",
     "start_time": "2023-09-15T17:02:26.447700337Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 299, 299, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer (QuantizeLa  (None, 299, 299, 3)          3         ['input_2[0][0]']             \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " quant_conv2d_94 (QuantizeW  (None, 149, 149, 32)         929       ['quantize_layer[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 149, 149, 32)         97        ['quant_conv2d_94[0][0]']     \n",
      " 94 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_activation_94 (Quant  (None, 149, 149, 32)         3         ['quant_batch_normalization_94\n",
      " izeWrapperV2)                                                      [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_95 (QuantizeW  (None, 147, 147, 32)         9281      ['quant_activation_94[0][0]'] \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 147, 147, 32)         97        ['quant_conv2d_95[0][0]']     \n",
      " 95 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_activation_95 (Quant  (None, 147, 147, 32)         3         ['quant_batch_normalization_95\n",
      " izeWrapperV2)                                                      [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_96 (QuantizeW  (None, 147, 147, 64)         18561     ['quant_activation_95[0][0]'] \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 147, 147, 64)         193       ['quant_conv2d_96[0][0]']     \n",
      " 96 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_activation_96 (Quant  (None, 147, 147, 64)         3         ['quant_batch_normalization_96\n",
      " izeWrapperV2)                                                      [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_4 (Qua  (None, 73, 73, 64)           1         ['quant_activation_96[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_97 (QuantizeW  (None, 73, 73, 80)           5281      ['quant_max_pooling2d_4[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 73, 73, 80)           241       ['quant_conv2d_97[0][0]']     \n",
      " 97 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_activation_97 (Quant  (None, 73, 73, 80)           3         ['quant_batch_normalization_97\n",
      " izeWrapperV2)                                                      [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_98 (QuantizeW  (None, 71, 71, 192)          138625    ['quant_activation_97[0][0]'] \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 71, 71, 192)          577       ['quant_conv2d_98[0][0]']     \n",
      " 98 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_activation_98 (Quant  (None, 71, 71, 192)          3         ['quant_batch_normalization_98\n",
      " izeWrapperV2)                                                      [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_5 (Qua  (None, 35, 35, 192)          1         ['quant_activation_98[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_102 (Quantize  (None, 35, 35, 64)           12417     ['quant_max_pooling2d_5[0][0]'\n",
      " WrapperV2)                                                         ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_102[0][0]']    \n",
      " 102 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_102 (Quan  (None, 35, 35, 64)           3         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_100 (Quantize  (None, 35, 35, 48)           9313      ['quant_max_pooling2d_5[0][0]'\n",
      " WrapperV2)                                                         ]                             \n",
      "                                                                                                  \n",
      " quant_conv2d_103 (Quantize  (None, 35, 35, 96)           55489     ['quant_activation_102[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 48)           145       ['quant_conv2d_100[0][0]']    \n",
      " 100 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 96)           289       ['quant_conv2d_103[0][0]']    \n",
      " 103 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_100 (Quan  (None, 35, 35, 48)           3         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_103 (Quan  (None, 35, 35, 96)           3         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_9   (None, 35, 35, 192)          3         ['quant_max_pooling2d_5[0][0]'\n",
      " (QuantizeWrapperV2)                                                ]                             \n",
      "                                                                                                  \n",
      " quant_conv2d_99 (QuantizeW  (None, 35, 35, 64)           12417     ['quant_max_pooling2d_5[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_conv2d_101 (Quantize  (None, 35, 35, 64)           76929     ['quant_activation_100[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_104 (Quantize  (None, 35, 35, 96)           83137     ['quant_activation_103[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_105 (Quantize  (None, 35, 35, 32)           6209      ['quant_average_pooling2d_9[0]\n",
      " WrapperV2)                                                         [0]']                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_99[0][0]']     \n",
      " 99 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_101[0][0]']    \n",
      " 101 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 96)           289       ['quant_conv2d_104[0][0]']    \n",
      " 104 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 32)           97        ['quant_conv2d_105[0][0]']    \n",
      " 105 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_99 (Quant  (None, 35, 35, 64)           1         ['quant_batch_normalization_99\n",
      " izeWrapperV2)                                                      [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_activation_101 (Quan  (None, 35, 35, 64)           1         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_104 (Quan  (None, 35, 35, 96)           1         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_105 (Quan  (None, 35, 35, 32)           1         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed0 (QuantizeWrap  (None, 35, 35, 256)          3         ['quant_activation_99[0][0]', \n",
      " perV2)                                                              'quant_activation_101[0][0]',\n",
      "                                                                     'quant_activation_104[0][0]',\n",
      "                                                                     'quant_activation_105[0][0]']\n",
      "                                                                                                  \n",
      " quant_conv2d_109 (Quantize  (None, 35, 35, 64)           16513     ['quant_mixed0[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_109[0][0]']    \n",
      " 109 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_109 (Quan  (None, 35, 35, 64)           3         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_107 (Quantize  (None, 35, 35, 48)           12385     ['quant_mixed0[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_110 (Quantize  (None, 35, 35, 96)           55489     ['quant_activation_109[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 48)           145       ['quant_conv2d_107[0][0]']    \n",
      " 107 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 96)           289       ['quant_conv2d_110[0][0]']    \n",
      " 110 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_107 (Quan  (None, 35, 35, 48)           3         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_110 (Quan  (None, 35, 35, 96)           3         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_10  (None, 35, 35, 256)          3         ['quant_mixed0[0][0]']        \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " quant_conv2d_106 (Quantize  (None, 35, 35, 64)           16513     ['quant_mixed0[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_108 (Quantize  (None, 35, 35, 64)           76929     ['quant_activation_107[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_111 (Quantize  (None, 35, 35, 96)           83137     ['quant_activation_110[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_112 (Quantize  (None, 35, 35, 64)           16513     ['quant_average_pooling2d_10[0\n",
      " WrapperV2)                                                         ][0]']                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_106[0][0]']    \n",
      " 106 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_108[0][0]']    \n",
      " 108 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 96)           289       ['quant_conv2d_111[0][0]']    \n",
      " 111 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_112[0][0]']    \n",
      " 112 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_106 (Quan  (None, 35, 35, 64)           1         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_108 (Quan  (None, 35, 35, 64)           1         ['quant_batch_normalization_10\n",
      " tizeWrapperV2)                                                     8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_111 (Quan  (None, 35, 35, 96)           1         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_112 (Quan  (None, 35, 35, 64)           1         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed1 (QuantizeWrap  (None, 35, 35, 288)          3         ['quant_activation_106[0][0]',\n",
      " perV2)                                                              'quant_activation_108[0][0]',\n",
      "                                                                     'quant_activation_111[0][0]',\n",
      "                                                                     'quant_activation_112[0][0]']\n",
      "                                                                                                  \n",
      " quant_conv2d_116 (Quantize  (None, 35, 35, 64)           18561     ['quant_mixed1[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_116[0][0]']    \n",
      " 116 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_116 (Quan  (None, 35, 35, 64)           3         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_114 (Quantize  (None, 35, 35, 48)           13921     ['quant_mixed1[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_117 (Quantize  (None, 35, 35, 96)           55489     ['quant_activation_116[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 48)           145       ['quant_conv2d_114[0][0]']    \n",
      " 114 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 96)           289       ['quant_conv2d_117[0][0]']    \n",
      " 117 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_114 (Quan  (None, 35, 35, 48)           3         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_117 (Quan  (None, 35, 35, 96)           3         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_11  (None, 35, 35, 288)          3         ['quant_mixed1[0][0]']        \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " quant_conv2d_113 (Quantize  (None, 35, 35, 64)           18561     ['quant_mixed1[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_115 (Quantize  (None, 35, 35, 64)           76929     ['quant_activation_114[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_118 (Quantize  (None, 35, 35, 96)           83137     ['quant_activation_117[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_119 (Quantize  (None, 35, 35, 64)           18561     ['quant_average_pooling2d_11[0\n",
      " WrapperV2)                                                         ][0]']                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_113[0][0]']    \n",
      " 113 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_115[0][0]']    \n",
      " 115 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 96)           289       ['quant_conv2d_118[0][0]']    \n",
      " 118 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_119[0][0]']    \n",
      " 119 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_113 (Quan  (None, 35, 35, 64)           1         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_115 (Quan  (None, 35, 35, 64)           1         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_118 (Quan  (None, 35, 35, 96)           1         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_119 (Quan  (None, 35, 35, 64)           1         ['quant_batch_normalization_11\n",
      " tizeWrapperV2)                                                     9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed2 (QuantizeWrap  (None, 35, 35, 288)          3         ['quant_activation_113[0][0]',\n",
      " perV2)                                                              'quant_activation_115[0][0]',\n",
      "                                                                     'quant_activation_118[0][0]',\n",
      "                                                                     'quant_activation_119[0][0]']\n",
      "                                                                                                  \n",
      " quant_conv2d_121 (Quantize  (None, 35, 35, 64)           18561     ['quant_mixed2[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 64)           193       ['quant_conv2d_121[0][0]']    \n",
      " 121 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_121 (Quan  (None, 35, 35, 64)           3         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_122 (Quantize  (None, 35, 35, 96)           55489     ['quant_activation_121[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 35, 35, 96)           289       ['quant_conv2d_122[0][0]']    \n",
      " 122 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_122 (Quan  (None, 35, 35, 96)           3         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_120 (Quantize  (None, 17, 17, 384)          996097    ['quant_mixed2[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_123 (Quantize  (None, 17, 17, 96)           83137     ['quant_activation_122[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 384)          1153      ['quant_conv2d_120[0][0]']    \n",
      " 120 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 96)           289       ['quant_conv2d_123[0][0]']    \n",
      " 123 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_6 (Qua  (None, 17, 17, 288)          1         ['quant_mixed2[0][0]']        \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_activation_120 (Quan  (None, 17, 17, 384)          1         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_123 (Quan  (None, 17, 17, 96)           1         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed3 (QuantizeWrap  (None, 17, 17, 768)          3         ['quant_max_pooling2d_6[0][0]'\n",
      " perV2)                                                             , 'quant_activation_120[0][0]'\n",
      "                                                                    , 'quant_activation_123[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " quant_conv2d_128 (Quantize  (None, 17, 17, 128)          98561     ['quant_mixed3[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 128)          385       ['quant_conv2d_128[0][0]']    \n",
      " 128 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_128 (Quan  (None, 17, 17, 128)          3         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_129 (Quantize  (None, 17, 17, 128)          114945    ['quant_activation_128[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 128)          385       ['quant_conv2d_129[0][0]']    \n",
      " 129 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_129 (Quan  (None, 17, 17, 128)          3         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_125 (Quantize  (None, 17, 17, 128)          98561     ['quant_mixed3[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_130 (Quantize  (None, 17, 17, 128)          114945    ['quant_activation_129[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 128)          385       ['quant_conv2d_125[0][0]']    \n",
      " 125 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 128)          385       ['quant_conv2d_130[0][0]']    \n",
      " 130 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_125 (Quan  (None, 17, 17, 128)          3         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_130 (Quan  (None, 17, 17, 128)          3         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_126 (Quantize  (None, 17, 17, 128)          114945    ['quant_activation_125[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_131 (Quantize  (None, 17, 17, 128)          114945    ['quant_activation_130[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 128)          385       ['quant_conv2d_126[0][0]']    \n",
      " 126 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 128)          385       ['quant_conv2d_131[0][0]']    \n",
      " 131 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_126 (Quan  (None, 17, 17, 128)          3         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_131 (Quan  (None, 17, 17, 128)          3         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_12  (None, 17, 17, 768)          3         ['quant_mixed3[0][0]']        \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " quant_conv2d_124 (Quantize  (None, 17, 17, 192)          147841    ['quant_mixed3[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_127 (Quantize  (None, 17, 17, 192)          172417    ['quant_activation_126[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_132 (Quantize  (None, 17, 17, 192)          172417    ['quant_activation_131[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_133 (Quantize  (None, 17, 17, 192)          147841    ['quant_average_pooling2d_12[0\n",
      " WrapperV2)                                                         ][0]']                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_124[0][0]']    \n",
      " 124 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_127[0][0]']    \n",
      " 127 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_132[0][0]']    \n",
      " 132 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_133[0][0]']    \n",
      " 133 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_124 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_127 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_12\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_132 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_133 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed4 (QuantizeWrap  (None, 17, 17, 768)          3         ['quant_activation_124[0][0]',\n",
      " perV2)                                                              'quant_activation_127[0][0]',\n",
      "                                                                     'quant_activation_132[0][0]',\n",
      "                                                                     'quant_activation_133[0][0]']\n",
      "                                                                                                  \n",
      " quant_conv2d_138 (Quantize  (None, 17, 17, 160)          123201    ['quant_mixed4[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_138[0][0]']    \n",
      " 138 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_138 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_139 (Quantize  (None, 17, 17, 160)          179521    ['quant_activation_138[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_139[0][0]']    \n",
      " 139 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_139 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_135 (Quantize  (None, 17, 17, 160)          123201    ['quant_mixed4[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_140 (Quantize  (None, 17, 17, 160)          179521    ['quant_activation_139[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_135[0][0]']    \n",
      " 135 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_140[0][0]']    \n",
      " 140 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_135 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_140 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_136 (Quantize  (None, 17, 17, 160)          179521    ['quant_activation_135[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_141 (Quantize  (None, 17, 17, 160)          179521    ['quant_activation_140[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_136[0][0]']    \n",
      " 136 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_141[0][0]']    \n",
      " 141 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_136 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_141 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_13  (None, 17, 17, 768)          3         ['quant_mixed4[0][0]']        \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " quant_conv2d_134 (Quantize  (None, 17, 17, 192)          147841    ['quant_mixed4[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_137 (Quantize  (None, 17, 17, 192)          215425    ['quant_activation_136[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_142 (Quantize  (None, 17, 17, 192)          215425    ['quant_activation_141[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_143 (Quantize  (None, 17, 17, 192)          147841    ['quant_average_pooling2d_13[0\n",
      " WrapperV2)                                                         ][0]']                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_134[0][0]']    \n",
      " 134 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_137[0][0]']    \n",
      " 137 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_142[0][0]']    \n",
      " 142 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_143[0][0]']    \n",
      " 143 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_134 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_137 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_13\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_142 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_143 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed5 (QuantizeWrap  (None, 17, 17, 768)          3         ['quant_activation_134[0][0]',\n",
      " perV2)                                                              'quant_activation_137[0][0]',\n",
      "                                                                     'quant_activation_142[0][0]',\n",
      "                                                                     'quant_activation_143[0][0]']\n",
      "                                                                                                  \n",
      " quant_conv2d_148 (Quantize  (None, 17, 17, 160)          123201    ['quant_mixed5[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_148[0][0]']    \n",
      " 148 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_148 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_149 (Quantize  (None, 17, 17, 160)          179521    ['quant_activation_148[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_149[0][0]']    \n",
      " 149 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_149 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_145 (Quantize  (None, 17, 17, 160)          123201    ['quant_mixed5[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_150 (Quantize  (None, 17, 17, 160)          179521    ['quant_activation_149[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_145[0][0]']    \n",
      " 145 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_150[0][0]']    \n",
      " 150 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_145 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_150 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_146 (Quantize  (None, 17, 17, 160)          179521    ['quant_activation_145[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_151 (Quantize  (None, 17, 17, 160)          179521    ['quant_activation_150[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_146[0][0]']    \n",
      " 146 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 160)          481       ['quant_conv2d_151[0][0]']    \n",
      " 151 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_146 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_151 (Quan  (None, 17, 17, 160)          3         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_14  (None, 17, 17, 768)          3         ['quant_mixed5[0][0]']        \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " quant_conv2d_144 (Quantize  (None, 17, 17, 192)          147841    ['quant_mixed5[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_147 (Quantize  (None, 17, 17, 192)          215425    ['quant_activation_146[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_152 (Quantize  (None, 17, 17, 192)          215425    ['quant_activation_151[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_153 (Quantize  (None, 17, 17, 192)          147841    ['quant_average_pooling2d_14[0\n",
      " WrapperV2)                                                         ][0]']                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_144[0][0]']    \n",
      " 144 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_147[0][0]']    \n",
      " 147 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_152[0][0]']    \n",
      " 152 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_153[0][0]']    \n",
      " 153 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_144 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_147 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_14\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_152 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_153 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed6 (QuantizeWrap  (None, 17, 17, 768)          3         ['quant_activation_144[0][0]',\n",
      " perV2)                                                              'quant_activation_147[0][0]',\n",
      "                                                                     'quant_activation_152[0][0]',\n",
      "                                                                     'quant_activation_153[0][0]']\n",
      "                                                                                                  \n",
      " quant_conv2d_158 (Quantize  (None, 17, 17, 192)          147841    ['quant_mixed6[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_158[0][0]']    \n",
      " 158 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_158 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_159 (Quantize  (None, 17, 17, 192)          258433    ['quant_activation_158[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_159[0][0]']    \n",
      " 159 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_159 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_155 (Quantize  (None, 17, 17, 192)          147841    ['quant_mixed6[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_160 (Quantize  (None, 17, 17, 192)          258433    ['quant_activation_159[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_155[0][0]']    \n",
      " 155 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_160[0][0]']    \n",
      " 160 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_155 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_160 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_156 (Quantize  (None, 17, 17, 192)          258433    ['quant_activation_155[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_161 (Quantize  (None, 17, 17, 192)          258433    ['quant_activation_160[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_156[0][0]']    \n",
      " 156 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_161[0][0]']    \n",
      " 161 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_156 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_161 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_15  (None, 17, 17, 768)          3         ['quant_mixed6[0][0]']        \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " quant_conv2d_154 (Quantize  (None, 17, 17, 192)          147841    ['quant_mixed6[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_157 (Quantize  (None, 17, 17, 192)          258433    ['quant_activation_156[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_162 (Quantize  (None, 17, 17, 192)          258433    ['quant_activation_161[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_163 (Quantize  (None, 17, 17, 192)          147841    ['quant_average_pooling2d_15[0\n",
      " WrapperV2)                                                         ][0]']                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_154[0][0]']    \n",
      " 154 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_157[0][0]']    \n",
      " 157 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_162[0][0]']    \n",
      " 162 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_163[0][0]']    \n",
      " 163 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_154 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_157 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_15\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_162 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_163 (Quan  (None, 17, 17, 192)          1         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed7 (QuantizeWrap  (None, 17, 17, 768)          3         ['quant_activation_154[0][0]',\n",
      " perV2)                                                              'quant_activation_157[0][0]',\n",
      "                                                                     'quant_activation_162[0][0]',\n",
      "                                                                     'quant_activation_163[0][0]']\n",
      "                                                                                                  \n",
      " quant_conv2d_166 (Quantize  (None, 17, 17, 192)          147841    ['quant_mixed7[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_166[0][0]']    \n",
      " 166 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_166 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_167 (Quantize  (None, 17, 17, 192)          258433    ['quant_activation_166[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_167[0][0]']    \n",
      " 167 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_167 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_164 (Quantize  (None, 17, 17, 192)          147841    ['quant_mixed7[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_168 (Quantize  (None, 17, 17, 192)          258433    ['quant_activation_167[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_164[0][0]']    \n",
      " 164 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 17, 17, 192)          577       ['quant_conv2d_168[0][0]']    \n",
      " 168 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_164 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_168 (Quan  (None, 17, 17, 192)          3         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_165 (Quantize  (None, 8, 8, 320)            553601    ['quant_activation_164[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_169 (Quantize  (None, 8, 8, 192)            332161    ['quant_activation_168[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 320)            961       ['quant_conv2d_165[0][0]']    \n",
      " 165 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 192)            577       ['quant_conv2d_169[0][0]']    \n",
      " 169 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_7 (Qua  (None, 8, 8, 768)            1         ['quant_mixed7[0][0]']        \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_activation_165 (Quan  (None, 8, 8, 320)            1         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_169 (Quan  (None, 8, 8, 192)            1         ['quant_batch_normalization_16\n",
      " tizeWrapperV2)                                                     9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed8 (QuantizeWrap  (None, 8, 8, 1280)           3         ['quant_max_pooling2d_7[0][0]'\n",
      " perV2)                                                             , 'quant_activation_165[0][0]'\n",
      "                                                                    , 'quant_activation_169[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " quant_conv2d_174 (Quantize  (None, 8, 8, 448)            574337    ['quant_mixed8[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 448)            1345      ['quant_conv2d_174[0][0]']    \n",
      " 174 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_174 (Quan  (None, 8, 8, 448)            3         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_171 (Quantize  (None, 8, 8, 384)            492289    ['quant_mixed8[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_175 (Quantize  (None, 8, 8, 384)            1549057   ['quant_activation_174[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_171[0][0]']    \n",
      " 171 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_175[0][0]']    \n",
      " 175 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_171 (Quan  (None, 8, 8, 384)            3         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_175 (Quan  (None, 8, 8, 384)            3         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_172 (Quantize  (None, 8, 8, 384)            443137    ['quant_activation_171[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_173 (Quantize  (None, 8, 8, 384)            443137    ['quant_activation_171[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_176 (Quantize  (None, 8, 8, 384)            443137    ['quant_activation_175[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_177 (Quantize  (None, 8, 8, 384)            443137    ['quant_activation_175[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_16  (None, 8, 8, 1280)           3         ['quant_mixed8[0][0]']        \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_172[0][0]']    \n",
      " 172 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_173[0][0]']    \n",
      " 173 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_176[0][0]']    \n",
      " 176 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_177[0][0]']    \n",
      " 177 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_170 (Quantize  (None, 8, 8, 320)            410241    ['quant_mixed8[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_178 (Quantize  (None, 8, 8, 192)            246145    ['quant_average_pooling2d_16[0\n",
      " WrapperV2)                                                         ][0]']                        \n",
      "                                                                                                  \n",
      " quant_activation_172 (Quan  (None, 8, 8, 384)            1         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_173 (Quan  (None, 8, 8, 384)            1         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_176 (Quan  (None, 8, 8, 384)            1         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_177 (Quan  (None, 8, 8, 384)            1         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 320)            961       ['quant_conv2d_170[0][0]']    \n",
      " 170 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 192)            577       ['quant_conv2d_178[0][0]']    \n",
      " 178 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_mixed9_0 (QuantizeWr  (None, 8, 8, 768)            1         ['quant_activation_172[0][0]',\n",
      " apperV2)                                                            'quant_activation_173[0][0]']\n",
      "                                                                                                  \n",
      " quant_concatenate_2 (Quant  (None, 8, 8, 768)            1         ['quant_activation_176[0][0]',\n",
      " izeWrapperV2)                                                       'quant_activation_177[0][0]']\n",
      "                                                                                                  \n",
      " quant_activation_170 (Quan  (None, 8, 8, 320)            1         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_178 (Quan  (None, 8, 8, 192)            1         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed9 (QuantizeWrap  (None, 8, 8, 2048)           3         ['quant_mixed9_0[0][0]',      \n",
      " perV2)                                                              'quant_concatenate_2[0][0]', \n",
      "                                                                     'quant_activation_170[0][0]',\n",
      "                                                                     'quant_activation_178[0][0]']\n",
      "                                                                                                  \n",
      " quant_conv2d_183 (Quantize  (None, 8, 8, 448)            918401    ['quant_mixed9[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 448)            1345      ['quant_conv2d_183[0][0]']    \n",
      " 183 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_183 (Quan  (None, 8, 8, 448)            3         ['quant_batch_normalization_18\n",
      " tizeWrapperV2)                                                     3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_180 (Quantize  (None, 8, 8, 384)            787201    ['quant_mixed9[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_184 (Quantize  (None, 8, 8, 384)            1549057   ['quant_activation_183[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_180[0][0]']    \n",
      " 180 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_184[0][0]']    \n",
      " 184 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_activation_180 (Quan  (None, 8, 8, 384)            3         ['quant_batch_normalization_18\n",
      " tizeWrapperV2)                                                     0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_184 (Quan  (None, 8, 8, 384)            3         ['quant_batch_normalization_18\n",
      " tizeWrapperV2)                                                     4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_181 (Quantize  (None, 8, 8, 384)            443137    ['quant_activation_180[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_182 (Quantize  (None, 8, 8, 384)            443137    ['quant_activation_180[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_185 (Quantize  (None, 8, 8, 384)            443137    ['quant_activation_184[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_186 (Quantize  (None, 8, 8, 384)            443137    ['quant_activation_184[0][0]']\n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_17  (None, 8, 8, 2048)           3         ['quant_mixed9[0][0]']        \n",
      "  (QuantizeWrapperV2)                                                                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_181[0][0]']    \n",
      " 181 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_182[0][0]']    \n",
      " 182 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_185[0][0]']    \n",
      " 185 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 384)            1153      ['quant_conv2d_186[0][0]']    \n",
      " 186 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_179 (Quantize  (None, 8, 8, 320)            656001    ['quant_mixed9[0][0]']        \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_187 (Quantize  (None, 8, 8, 192)            393601    ['quant_average_pooling2d_17[0\n",
      " WrapperV2)                                                         ][0]']                        \n",
      "                                                                                                  \n",
      " quant_activation_181 (Quan  (None, 8, 8, 384)            1         ['quant_batch_normalization_18\n",
      " tizeWrapperV2)                                                     1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_182 (Quan  (None, 8, 8, 384)            1         ['quant_batch_normalization_18\n",
      " tizeWrapperV2)                                                     2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_185 (Quan  (None, 8, 8, 384)            1         ['quant_batch_normalization_18\n",
      " tizeWrapperV2)                                                     5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_186 (Quan  (None, 8, 8, 384)            1         ['quant_batch_normalization_18\n",
      " tizeWrapperV2)                                                     6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 320)            961       ['quant_conv2d_179[0][0]']    \n",
      " 179 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 8, 8, 192)            577       ['quant_conv2d_187[0][0]']    \n",
      " 187 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_mixed9_1 (QuantizeWr  (None, 8, 8, 768)            1         ['quant_activation_181[0][0]',\n",
      " apperV2)                                                            'quant_activation_182[0][0]']\n",
      "                                                                                                  \n",
      " quant_concatenate_3 (Quant  (None, 8, 8, 768)            1         ['quant_activation_185[0][0]',\n",
      " izeWrapperV2)                                                       'quant_activation_186[0][0]']\n",
      "                                                                                                  \n",
      " quant_activation_179 (Quan  (None, 8, 8, 320)            1         ['quant_batch_normalization_17\n",
      " tizeWrapperV2)                                                     9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_activation_187 (Quan  (None, 8, 8, 192)            1         ['quant_batch_normalization_18\n",
      " tizeWrapperV2)                                                     7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_mixed10 (QuantizeWra  (None, 8, 8, 2048)           3         ['quant_mixed9_1[0][0]',      \n",
      " pperV2)                                                             'quant_concatenate_3[0][0]', \n",
      "                                                                     'quant_activation_179[0][0]',\n",
      "                                                                     'quant_activation_187[0][0]']\n",
      "                                                                                                  \n",
      " quant_flatten (QuantizeWra  (None, 131072)               1         ['quant_mixed10[0][0]']       \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_dense (QuantizeWrapp  (None, 256)                  3355469   ['quant_flatten[0][0]']       \n",
      " erV2)                                                    3                                       \n",
      "                                                                                                  \n",
      " quant_dense_1 (QuantizeWra  (None, 150)                  38555     ['quant_dense[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55430918 (211.45 MB)\n",
      "Trainable params: 55361590 (211.19 MB)\n",
      "Non-trainable params: 69328 (270.81 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hist = q_model.fit(train_generator, validation_data=val_generator, epochs=50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQ8kL4pLXugN",
    "outputId": "3ef64484-f6e7-4d9b-bb71-c2518a0cd3db",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:08:21.818458874Z",
     "start_time": "2023-09-15T17:02:26.790800718Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 22:32:40.827417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2023-09-15 22:32:40.906163: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-15 22:32:43.453989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-15 22:32:43.469585: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f11349af390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-15 22:32:43.469606: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-09-15 22:32:43.472561: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-15 22:32:43.515654: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-15 22:32:43.552245: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 41s 855ms/step - loss: 0.3421 - mae: 0.2285 - val_loss: 0.2086 - val_mae: 0.1161\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 6s 549ms/step - loss: 0.2632 - mae: 0.1601 - val_loss: 0.1858 - val_mae: 0.0965\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 6s 556ms/step - loss: 0.2599 - mae: 0.1560 - val_loss: 0.1846 - val_mae: 0.0960\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 6s 570ms/step - loss: 0.2655 - mae: 0.1567 - val_loss: 0.1850 - val_mae: 0.0913\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 6s 557ms/step - loss: 0.2641 - mae: 0.1548 - val_loss: 0.1828 - val_mae: 0.0882\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 6s 572ms/step - loss: 0.2653 - mae: 0.1546 - val_loss: 0.1814 - val_mae: 0.0859\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 6s 555ms/step - loss: 0.2693 - mae: 0.1556 - val_loss: 0.1805 - val_mae: 0.0838\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 6s 565ms/step - loss: 0.2660 - mae: 0.1518 - val_loss: 0.1799 - val_mae: 0.0832\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 6s 554ms/step - loss: 0.2706 - mae: 0.1537 - val_loss: 0.1981 - val_mae: 0.0905\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 6s 576ms/step - loss: 0.2731 - mae: 0.1550 - val_loss: 0.1953 - val_mae: 0.0877\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 6s 572ms/step - loss: 0.2733 - mae: 0.1530 - val_loss: 0.1924 - val_mae: 0.0874\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 6s 550ms/step - loss: 0.2777 - mae: 0.1558 - val_loss: 0.1904 - val_mae: 0.0853\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 6s 556ms/step - loss: 0.2773 - mae: 0.1553 - val_loss: 0.1905 - val_mae: 0.0831\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 6s 570ms/step - loss: 0.2824 - mae: 0.1580 - val_loss: 0.1973 - val_mae: 0.0849\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 6s 564ms/step - loss: 0.2833 - mae: 0.1576 - val_loss: 0.1994 - val_mae: 0.0866\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 6s 556ms/step - loss: 0.2838 - mae: 0.1581 - val_loss: 0.1938 - val_mae: 0.0846\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 6s 562ms/step - loss: 0.2844 - mae: 0.1593 - val_loss: 0.2092 - val_mae: 0.0914\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 6s 563ms/step - loss: 0.2870 - mae: 0.1606 - val_loss: 0.2074 - val_mae: 0.0927\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 6s 547ms/step - loss: 0.2886 - mae: 0.1606 - val_loss: 0.2077 - val_mae: 0.0895\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 6s 544ms/step - loss: 0.2911 - mae: 0.1612 - val_loss: 0.2164 - val_mae: 0.0939\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 6s 559ms/step - loss: 0.2964 - mae: 0.1647 - val_loss: 0.2196 - val_mae: 0.0964\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 6s 554ms/step - loss: 0.2990 - mae: 0.1659 - val_loss: 0.2264 - val_mae: 0.0999\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 7s 548ms/step - loss: 0.3046 - mae: 0.1695 - val_loss: 0.2425 - val_mae: 0.1072\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 7s 568ms/step - loss: 0.3107 - mae: 0.1734 - val_loss: 0.2375 - val_mae: 0.1056\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 7s 589ms/step - loss: 0.3103 - mae: 0.1724 - val_loss: 0.2280 - val_mae: 0.1014\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 6s 544ms/step - loss: 0.3115 - mae: 0.1737 - val_loss: 0.2335 - val_mae: 0.1041\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 6s 556ms/step - loss: 0.3135 - mae: 0.1750 - val_loss: 0.2364 - val_mae: 0.1054\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 6s 560ms/step - loss: 0.3157 - mae: 0.1760 - val_loss: 0.2383 - val_mae: 0.1064\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 6s 553ms/step - loss: 0.3184 - mae: 0.1778 - val_loss: 0.2425 - val_mae: 0.1088\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 6s 553ms/step - loss: 0.3200 - mae: 0.1794 - val_loss: 0.2416 - val_mae: 0.1082\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 6s 553ms/step - loss: 0.3220 - mae: 0.1811 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 7s 579ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 6s 571ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 6s 560ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 6s 571ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 6s 559ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 6s 565ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 6s 552ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 6s 543ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 6s 562ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 6s 546ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 7s 529ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 7s 572ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 7s 558ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 6s 560ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 7s 580ms/step - loss: 0.3232 - mae: 0.1820 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 6s 574ms/step - loss: 0.3232 - mae: 0.1821 - val_loss: 0.2446 - val_mae: 0.1107\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 7s 539ms/step - loss: 0.3229 - mae: 0.1819 - val_loss: 0.2479 - val_mae: 0.1117\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "q_model.save(\"q_model\")"
   ],
   "metadata": {
    "id": "xrWG3VxxZBcW",
    "ExecuteTime": {
     "end_time": "2023-09-15T17:08:51.672024376Z",
     "start_time": "2023-09-15T17:08:21.829665516Z"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: q_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: q_model/assets\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from tensorflow import lite"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T17:18:03.582936955Z",
     "start_time": "2023-09-15T17:18:03.542134916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx11zt6s6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx11zt6s6/assets\n",
      "/home/gk/python-envs/scratch/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-09-15 22:50:08.925962: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-09-15 22:50:08.925985: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-09-15 22:50:08.926618: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpx11zt6s6\n",
      "2023-09-15 22:50:08.993181: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-09-15 22:50:08.993211: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpx11zt6s6\n",
      "2023-09-15 22:50:09.149900: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-09-15 22:50:09.215677: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-09-15 22:50:10.873927: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpx11zt6s6\n",
      "2023-09-15 22:50:11.348253: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 2421636 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T17:20:16.587605570Z",
     "start_time": "2023-09-15T17:19:22.738541255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "[0.99609375 0.         0.99609375 0.         0.99609375 0.99609375\n",
      " 0.         0.99609375 0.         0.99609375 0.99609375 0.\n",
      " 0.99609375 0.         0.99609375 0.99609375 0.99609375 0.99609375\n",
      " 0.         0.99609375 0.99609375 0.         0.99609375 0.\n",
      " 0.         0.99609375 0.99609375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.99609375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    for i in val_generator:\n",
    "        for j in range(len(i[0])):\n",
    "            image = np.expand_dims(i[0][j], axis=0)\n",
    "            target = i[1][j]\n",
    "            interpreter.set_tensor(input_index, image)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.tensor(output_index)\n",
    "            print(output()[0])\n",
    "    # for i, test_image in enumerate(val_dataset):\n",
    "    #   if i % 1000 == 0:\n",
    "    #     print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    #   # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    #   # the model's input data format.\n",
    "    #   test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    #   interpreter.set_tensor(input_index, test_image)\n",
    "    # \n",
    "    #   # Run inference.\n",
    "    #   interpreter.invoke()\n",
    "    # \n",
    "    #   # Post-processing: remove batch dimension and find the digit with highest\n",
    "    #   # probability.\n",
    "    #   output = interpreter.tensor(output_index)\n",
    "    #   digit = np.argmax(output()[0])\n",
    "    #   prediction_digits.append(digit)\n",
    "    # \n",
    "    # print('\\n')\n",
    "    # # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    # prediction_digits = np.array(prediction_digits)\n",
    "    # accuracy = (prediction_digits == test_labels).mean()\n",
    "    # return accuracy\n",
    "\n",
    "\n",
    "evaluate_model(interpreter)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T17:37:21.682030536Z",
     "start_time": "2023-09-15T17:37:20.018927264Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# test_accuracy = evaluate_model(interpreter)\n",
    "# \n",
    "# print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "# print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T17:25:24.066743653Z",
     "start_time": "2023-09-15T17:25:24.025181302Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/images/val/273271,1b9eb00089049cd6.jpg\n",
      "data/images/val/273271,1bb400001cf13c4b.jpg\n",
      "data/images/val/273271,1bab200041e8121f.jpg\n",
      "data/images/val/273271,1c3a7000ee250436.jpg\n",
      "data/images/val/273271,1a27b000f0c7a077.jpg\n",
      "data/images/val/273271,1c02a00058d9f56e.jpg\n",
      "data/images/val/273271,1bcf6000b9a2c34e.jpg\n",
      "data/images/val/273271,1bdb20008371af00.jpg\n",
      "data/images/val/273271,1c5ed000d09f3a10.jpg\n",
      "data/images/val/273271,1b86f000bc5b77bf.jpg\n",
      "data/images/val/273271,1bc75000ad1a58db.jpg\n",
      "data/images/val/273271,1ba93000e00e011c.jpg\n",
      "data/images/val/273271,1a02900084ed5ae8.jpg\n",
      "data/images/val/273271,1c3c10003afd20b9.jpg\n",
      "data/images/val/273271,1ba470008722b054.jpg\n",
      "data/images/val/273271,1a51000317893a5.jpg\n",
      "data/images/val/273271,1b9eb00089049cd6.jpg\n",
      "data/images/val/273271,1bb400001cf13c4b.jpg\n",
      "data/images/val/273271,1bab200041e8121f.jpg\n",
      "data/images/val/273271,1c3a7000ee250436.jpg\n",
      "data/images/val/273271,1a27b000f0c7a077.jpg\n",
      "data/images/val/273271,1c02a00058d9f56e.jpg\n",
      "data/images/val/273271,1bcf6000b9a2c34e.jpg\n",
      "data/images/val/273271,1bdb20008371af00.jpg\n",
      "data/images/val/273271,1c5ed000d09f3a10.jpg\n",
      "data/images/val/273271,1b86f000bc5b77bf.jpg\n",
      "data/images/val/273271,1bc75000ad1a58db.jpg\n",
      "data/images/val/273271,1ba93000e00e011c.jpg\n",
      "data/images/val/273271,1a02900084ed5ae8.jpg\n",
      "data/images/val/273271,1c3c10003afd20b9.jpg\n",
      "data/images/val/273271,1ba470008722b054.jpg\n",
      "data/images/val/273271,1a51000317893a5.jpg\n",
      "1/1 [==============================] - 0s 207ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = q_model.predict(val_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T17:46:22.545575221Z",
     "start_time": "2023-09-15T17:46:22.108969851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00004357758371043018996715545654296875000000000000 1.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000 1.00000000000000000000000000000000000000000000000000\n",
      "0.00000000000000000000000000000000000000000000000000 1.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000 1.00000000000000000000000000000000000000000000000000\n",
      "0.00000000000000000000000000000009782478093475286509 1.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000 1.00000000000000000000000000000000000000000000000000\n",
      "1.00000000000000000000000000000000000000000000000000 1.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000 1.00000000000000000000000000000000000000000000000000\n",
      "0.00000000000000000000000000000000000000000000000000 1.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000\n",
      "1.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000001050511530275840169\n",
      "0.00000000000000000000000000000000036968096584248676 0.00000000000000010249447401123286459194428044128244 0.00000000000000000000000000000000000000000000000000 0.00000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "for i in pred:\n",
    "    bboxes = [bbox[1:] for bbox in i.reshape((30, 5)) if bbox[0] >= .5]\n",
    "    with open(\"output.txt\", \"w\") as file:\n",
    "        for row in bboxes:\n",
    "            row_str = \" \".join([f\"{item:.50f}\" for item in row])  # Format each number in the row with 2 decimal places and join with spaces\n",
    "            file.write(row_str + \"\\n\")  \n",
    "        file.close()\n",
    "    break\n",
    "# Open the file for reading\n",
    "with open(\"output.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        print(line.strip())  # Use strip() to remove leading/trailing whitespace and newline characters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T18:15:46.954785153Z",
     "start_time": "2023-09-15T18:15:46.911991821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
